{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caress'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('caresses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses --> caress\n",
      "ponies --> poni\n",
      "pony --> poni\n",
      "cats --> cat\n",
      "running --> run\n",
      "runner --> runner\n",
      "climber --> climber\n",
      "easily --> easili\n",
      "quickly --> quickli\n"
     ]
    }
   ],
   "source": [
    "sample_words = ['caresses', 'ponies', 'pony', 'cats', 'running', 'runner', 'climber', 'easily', 'quickly']\n",
    "\n",
    "for word in sample_words:\n",
    "    print(word, '-->', porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter2 Stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses --> caress\n",
      "ponies --> poni\n",
      "pony --> poni\n",
      "cats --> cat\n",
      "running --> run\n",
      "runner --> runner\n",
      "climber --> climber\n",
      "easily --> easili\n",
      "quickly --> quick\n"
     ]
    }
   ],
   "source": [
    "for word in sample_words:\n",
    "    print(word, '-->', snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_object = nlp(\"I like to jog. I hated it in my childhood though.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> pronoun\n",
      "like --> verb\n",
      "to --> particle\n",
      "jog --> verb\n",
      ". --> punctuation\n",
      "I --> pronoun\n",
      "hated --> verb\n",
      "it --> pronoun\n",
      "in --> adposition\n",
      "my --> adjective\n",
      "childhood --> noun\n",
      "though --> adverb\n",
      ". --> punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc_object:\n",
    "    print(token, '-->', spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> -PRON- --> None\n",
      "like --> like --> None\n",
      "to --> to --> None\n",
      "jog --> jog --> None\n",
      ". --> . --> punctuation mark, sentence closer\n",
      "I --> -PRON- --> None\n",
      "hated --> hat --> None\n",
      "it --> -PRON- --> None\n",
      "in --> in --> None\n",
      "my --> -PRON- --> None\n",
      "childhood --> childhood --> None\n",
      "though --> though --> None\n",
      ". --> . --> punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# examine root words\n",
    "for word in doc_object:\n",
    "    print(word.text, '-->', word.lemma_, '-->', spacy.explain(word.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses --> caress --> NOUN --> None\n",
      "ponies --> pony --> NOUN --> None\n",
      "pony --> pony --> NOUN --> None\n",
      "cats --> cat --> NOUN --> None\n",
      "running --> run --> VERB --> None\n",
      "runner --> runner --> NOUN --> None\n",
      "climber --> climber --> NOUN --> None\n",
      "easily --> easily --> ADV --> None\n",
      "quickly --> quickly --> ADV --> None\n"
     ]
    }
   ],
   "source": [
    "for word in sample_words:\n",
    "    obj = nlp(word)\n",
    "    for token in obj:\n",
    "        print(token.text, '-->', token.lemma_, '-->', token.pos_, '-->', spacy.explain(token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.16         \n",
      "    Location           C:\\Users\\L00151142\\.conda\\envs\\AI2_course\\lib\\site-packages\\spacy\n",
      "    Platform           Windows-10-10.0.18362-SP0\n",
      "    Python version     3.7.6          \n",
      "    Models                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "The --> the --> DET --> None\n",
      "brown --> brown --> ADJ --> None\n",
      "fox --> fox --> NOUN --> None\n",
      "is --> be --> VERB --> None\n",
      "quick --> quick --> ADJ --> None\n",
      "and --> and --> CCONJ --> None\n",
      "he --> -PRON- --> PRON --> None\n",
      "is --> be --> VERB --> None\n",
      "jumping --> jump --> VERB --> None\n",
      "over --> over --> ADP --> None\n",
      "the --> the --> DET --> None\n",
      "lazy --> lazy --> ADJ --> None\n",
      "dog --> dog --> NOUN --> None\n"
     ]
    }
   ],
   "source": [
    "# create a function that allows you to input some text and then output\n",
    "# the txt with its lemma, structure the output\n",
    "\n",
    "# The brown fox is quick and he is jumping over the lazy dog\n",
    "\n",
    "def lemmatize():\n",
    "    text = input()\n",
    "    nlp_text = nlp(text)\n",
    "    for word in nlp_text:\n",
    "        print(word.text, '-->', word.lemma_, '-->', word.pos_, '-->', spacy.explain(word.lemma_))\n",
    "        \n",
    "lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-seech (POS Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                    PRON                 PRP                  pronoun, personal   \n",
      "like                 VERB                 VBP                  verb, non-3rd person singular present\n",
      "to                   PART                 TO                   infinitival to      \n",
      "jog                  VERB                 VB                   verb, base form     \n",
      ".                    PUNCT                .                    punctuation mark, sentence closer\n",
      "I                    PRON                 PRP                  pronoun, personal   \n",
      "hated                VERB                 VBD                  verb, past tense    \n",
      "it                   PRON                 PRP                  pronoun, personal   \n",
      "in                   ADP                  IN                   conjunction, subordinating or preposition\n",
      "my                   ADJ                  PRP$                 pronoun, possessive \n",
      "childhood            NOUN                 NN                   noun, singular or mass\n",
      "though               ADV                  RB                   adverb              \n",
      ".                    PUNCT                .                    punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc_object:\n",
    "    print(f\"{token.text:{20}} {token.pos_:{20}} {token.tag_:{20}} {spacy.explain(token.tag_):{20}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_obj = nlp(\"Can you Google it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can                  VERB                 MD                   verb, modal auxiliary\n",
      "you                  PRON                 PRP                  pronoun, personal   \n",
      "Google               VERB                 VB                   verb, base form     \n",
      "it                   PRON                 PRP                  pronoun, personal   \n",
      "?                    PUNCT                .                    punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc_obj:\n",
    "    print(f\"{token.text:{20}} {token.pos_:{20}} {token.tag_:{20}} {spacy.explain(token.tag_):{20}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_obj = nlp(\"Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google               PROPN                NNP                  noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "for token in doc_obj:\n",
    "    print(f\"{token.text:{20}} {token.pos_:{20}} {token.tag_:{20}} {spacy.explain(token.tag_):{20}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 2, 99: 3, 84: 1, 83: 1, 85: 1, 91: 1, 93: 1, 94: 3}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_frequency = doc_object.count_by(spacy.attrs.POS)\n",
    "POS_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRON'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_object.vocab[94].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        96          2 PUNCT      punctuation\n",
      "        99          3 VERB       verb      \n",
      "        84          1 ADP        adposition\n",
      "        83          1 ADJ        adjective \n",
      "        85          1 ADV        adverb    \n",
      "        91          1 NOUN       noun      \n",
      "        93          1 PART       particle  \n",
      "        94          3 PRON       pronoun   \n"
     ]
    }
   ],
   "source": [
    "for tag_id, occurrences in POS_frequency.items():\n",
    "    print(f\"{tag_id:{10}} {occurrences:{10}} {doc_object.vocab[tag_id].text:{10}} {spacy.explain(doc_object.vocab[tag_id].text):{10}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = open(\"stop-words.txt\")\n",
    "sentence = file_name.read()\n",
    "doc_object = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words like \"a\" and \"the\" are called stop---words.\n",
      "Sometimes this can be written as stop-words or stopwords.\n",
      "Each stop word can be filtered from the text to be processed.\n",
      "spaCy holds a built-in list of some 305 English stop--words.\n"
     ]
    }
   ],
   "source": [
    "print(doc_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_match1 = [{\"LOWER\": \"stopword\"}]\n",
    "token_match2 = [{\"LOWER\": \"stopwords\"}]\n",
    "token_match3 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"word\"}]\n",
    "token_match4 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"word\"}]\n",
    "token_match5 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"word\"}]\n",
    "\n",
    "matcher.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "token_matches = matcher(doc_object)\n",
    "print(token_matches)\n",
    "for token in token_matches:\n",
    "    print(f\"{token}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
