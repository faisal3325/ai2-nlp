{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tsv file into a dataframe object\n",
    "# Press tab to check you are in the correct folder location and to browse\n",
    "# to the tsv file\n",
    "# The sep command indicates this files is separated by tabs\n",
    "dataframe = pd.read_csv(\"SMSSpamCollection.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_length_col = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    length_message_text = len(row.message)\n",
    "    # add the length of each message to list\n",
    "    message_length_col.append(length_message_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll add the contents of this list to a new column\n",
    "# called \"length\" to the end of our dataframe\n",
    "# See https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\n",
    "dataframe['length'] = message_length_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_data = []\n",
    "spam_data = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    # If the label data is recognised to be \"ham\"\n",
    "    if row[\"label\"] == \"ham\":\n",
    "        ham_data.append(row)\n",
    "    else:\n",
    "        spam_data.append(row)\n",
    "\n",
    "# Convert list to a dataframe before performing descriptive statistics on it\n",
    "ham_dataframe = pd.DataFrame(ham_data)\n",
    "spam_dataframe = pd.DataFrame(spam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables first\n",
    "punct_length_col = []\n",
    "punct_count = 0\n",
    "\n",
    "for index, textrow in dataframe.iterrows():\n",
    "    doc_object = nlp(textrow.message)\n",
    "    for word in doc_object:\n",
    "        if word.pos_ == 'PUNCT':           \n",
    "            punct_count += 1\n",
    "    # Sentence is checked so add count to list\n",
    "    punct_length_col.append(punct_count)\n",
    "    punct_count =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add punct list to dataframe\n",
    "dataframe['punct'] = punct_length_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      4\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      2\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      1\n",
       "3   ham  U dun say so early hor... U c already then say...      49      2\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View top of dataframe content\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the feature data\n",
    "# We're creating a list of column names to\n",
    "# use from our dataframe.\n",
    "# We need 2 brackets as there is more than 1 entry\n",
    "X = dataframe[['length', 'punct']]\n",
    "\n",
    "# This is the label data - 1 entry\n",
    "# so only need 1 set of brackets\n",
    "y = dataframe['label']\n",
    "\n",
    "# Use SHIFT + TAB to see full options and to\n",
    "# copy some contents below\n",
    "# test-size represents percentage to use for testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train data shape (3900, 2)\n",
      "X test data shape (1672, 2)\n"
     ]
    }
   ],
   "source": [
    "# Contains 2 columns\n",
    "print('X train data shape', X_train.shape)\n",
    "print('X test data shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train data shape (3900,)\n",
      "y test data shape (1672,)\n"
     ]
    }
   ],
   "source": [
    "# 1 column of label data\n",
    "print('y train data shape', y_train.shape)\n",
    "print('y test data shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4393     ham\n",
       "216      ham\n",
       "4471     ham\n",
       "3889     ham\n",
       "5030    spam\n",
       "2000     ham\n",
       "4135     ham\n",
       "1055     ham\n",
       "3646    spam\n",
       "4774     ham\n",
       "1758     ham\n",
       "4796     ham\n",
       "4857     ham\n",
       "5311     ham\n",
       "2879    spam\n",
       "1416     ham\n",
       "3680     ham\n",
       "460      ham\n",
       "3431     ham\n",
       "1784     ham\n",
       "4570     ham\n",
       "1135     ham\n",
       "267      ham\n",
       "1892     ham\n",
       "1926     ham\n",
       "3853     ham\n",
       "3663     ham\n",
       "1521    spam\n",
       "3225     ham\n",
       "4583     ham\n",
       "        ... \n",
       "1031     ham\n",
       "1110     ham\n",
       "1888    spam\n",
       "3550     ham\n",
       "1527     ham\n",
       "753      ham\n",
       "3049     ham\n",
       "2628     ham\n",
       "562      ham\n",
       "4764     ham\n",
       "3562    spam\n",
       "252      ham\n",
       "2516     ham\n",
       "2962     ham\n",
       "4453     ham\n",
       "5374     ham\n",
       "5396     ham\n",
       "1202     ham\n",
       "3462     ham\n",
       "2797     ham\n",
       "4225     ham\n",
       "144      ham\n",
       "5056     ham\n",
       "2895     ham\n",
       "2763     ham\n",
       "905      ham\n",
       "5192     ham\n",
       "3980     ham\n",
       "235     spam\n",
       "5157     ham\n",
       "Name: label, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index position matches with index\n",
    "# position in X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      length  punct\n",
      "1078      28      1\n",
      "4028      45      3\n",
      "958       26      0\n",
      "4642       7      1\n",
      "4674     107      5\n",
      "5461      51      3\n",
      "4210      74      5\n",
      "4216      26      1\n",
      "1603      25      2\n",
      "1504      31      2\n",
      "1783      53      2\n",
      "3465       8      0\n",
      "5534      28      0\n",
      "4267      85      1\n",
      "2498      51      2\n",
      "4259      29      1\n",
      "147      159      4\n",
      "141       33      1\n",
      "4517     161     10\n",
      "3053      49      2\n",
      "5392      59      0\n",
      "2346      41      2\n",
      "1242      59      2\n",
      "3224      33      0\n",
      "4872      35      2\n",
      "3044      42      1\n",
      "1660      28      2\n",
      "3214      14      1\n",
      "501      149      6\n",
      "1827     332     11\n",
      "...      ...    ...\n",
      "1673     157      4\n",
      "1433      38      1\n",
      "616      145      2\n",
      "3416      29      0\n",
      "4035      54      3\n",
      "1646      31      1\n",
      "1395      36      1\n",
      "630      148      2\n",
      "955       41      0\n",
      "194      111      2\n",
      "4392      85      1\n",
      "909       31      0\n",
      "5540     158      5\n",
      "1006      37      0\n",
      "5080      93      3\n",
      "4548     124      6\n",
      "5345      16      1\n",
      "4545      35      1\n",
      "368      129      3\n",
      "3677      53      2\n",
      "4692      51      2\n",
      "3531      87      4\n",
      "3409     157      1\n",
      "4964      34      1\n",
      "2332      18      0\n",
      "3954     114      6\n",
      "619       59      2\n",
      "1987      24      1\n",
      "2358      52      1\n",
      "3594      22      1\n",
      "\n",
      "[1672 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078     ham\n",
       "4028     ham\n",
       "958      ham\n",
       "4642     ham\n",
       "4674     ham\n",
       "5461     ham\n",
       "4210     ham\n",
       "4216     ham\n",
       "1603     ham\n",
       "1504     ham\n",
       "1783     ham\n",
       "3465     ham\n",
       "5534     ham\n",
       "4267     ham\n",
       "2498     ham\n",
       "4259     ham\n",
       "147     spam\n",
       "141      ham\n",
       "4517    spam\n",
       "3053     ham\n",
       "5392     ham\n",
       "2346     ham\n",
       "1242     ham\n",
       "3224     ham\n",
       "4872     ham\n",
       "3044     ham\n",
       "1660     ham\n",
       "3214     ham\n",
       "501      ham\n",
       "1827     ham\n",
       "        ... \n",
       "1673    spam\n",
       "1433     ham\n",
       "616      ham\n",
       "3416     ham\n",
       "4035     ham\n",
       "1646     ham\n",
       "1395     ham\n",
       "630     spam\n",
       "955     spam\n",
       "194      ham\n",
       "4392     ham\n",
       "909      ham\n",
       "5540    spam\n",
       "1006     ham\n",
       "5080     ham\n",
       "4548     ham\n",
       "5345     ham\n",
       "4545     ham\n",
       "368     spam\n",
       "3677     ham\n",
       "4692     ham\n",
       "3531     ham\n",
       "3409    spam\n",
       "4964     ham\n",
       "2332     ham\n",
       "3954    spam\n",
       "619      ham\n",
       "1987     ham\n",
       "2358     ham\n",
       "3594     ham\n",
       "Name: label, Length: 1672, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_model = LogisticRegression(solver='lbfgs')\n",
    "# Note that the \"fit\" option must be run in the same cell as line above\n",
    "lin_reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Create a prediction set:\n",
    "# The model has not yet seen contents of X_test\n",
    "# which is a dataset of message length and punctuation\n",
    "# And we know to expect answers in y_test\n",
    "# which is a list of expected label output\n",
    "lin_reg_model_predictions = lin_reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the predicted output from the model\n",
    "lin_reg_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1391   51]\n",
      " [ 220   10]]\n"
     ]
    }
   ],
   "source": [
    "# Now we compare what the model predicted \n",
    "# with what is expected as output\n",
    "# Print a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,lin_reg_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted ham</th>\n",
       "      <th>predicted spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct ham</th>\n",
       "      <td>1391</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct spam</th>\n",
       "      <td>220</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predicted ham  predicted spam\n",
       "correct ham            1391              51\n",
       "correct spam            220              10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "dataframe_labels = pd.DataFrame(metrics.confusion_matrix(y_test,lin_reg_model_predictions), \n",
    "                  index=['correct ham','correct spam'], \n",
    "                  columns=['predicted ham','predicted spam'])\n",
    "dataframe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      0.96      0.91      1442\n",
      "        spam       0.16      0.04      0.07       230\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1672\n",
      "   macro avg       0.51      0.50      0.49      1672\n",
      "weighted avg       0.77      0.84      0.80      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,lin_reg_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import the model we want to use\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create an instance of the model - common model for text data and spam filtering\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Fit model to training data\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1442    0]\n",
      " [ 230    0]]\n"
     ]
    }
   ],
   "source": [
    "# Predict answers to data from the X_text dataset\n",
    "# containing text length and punctuation count\n",
    "nb_model_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Show results in a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted ham</th>\n",
       "      <th>predicted spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct ham</th>\n",
       "      <td>1442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct spam</th>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predicted ham  predicted spam\n",
       "correct ham            1442               0\n",
       "correct spam            230               0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "dataframe_labels = pd.DataFrame(metrics.confusion_matrix(y_test,nb_model_predictions), \n",
    "                  index=['correct ham','correct spam'], \n",
    "                  columns=['predicted ham','predicted spam'])\n",
    "dataframe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      1.00      0.93      1442\n",
      "        spam       0.00      0.00      0.00       230\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1672\n",
      "   macro avg       0.43      0.50      0.46      1672\n",
      "weighted avg       0.74      0.86      0.80      1672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faisa\\Anaconda3\\envs\\AI2_course\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8624401913875598\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faisa\\Anaconda3\\envs\\AI2_course\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import the model we want to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the model - common model for text data and spam filtering\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit model to training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1359   83]\n",
      " [ 126  104]]\n"
     ]
    }
   ],
   "source": [
    "# Predict answers to data from the X_text dataset\n",
    "# containing text length and punctuation count\n",
    "rf_model_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Show results in a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted ham</th>\n",
       "      <th>predicted spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct ham</th>\n",
       "      <td>1364</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct spam</th>\n",
       "      <td>111</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predicted ham  predicted spam\n",
       "correct ham            1364              78\n",
       "correct spam            111             119"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "dataframe_labels = pd.DataFrame(metrics.confusion_matrix(y_test,rf_model_predictions), \n",
    "                  index=['correct ham','correct spam'], \n",
    "                  columns=['predicted ham','predicted spam'])\n",
    "dataframe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      0.94      0.93      1442\n",
      "        spam       0.56      0.45      0.50       230\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1672\n",
      "   macro avg       0.74      0.70      0.71      1672\n",
      "weighted avg       0.87      0.88      0.87      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
